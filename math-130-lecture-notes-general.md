# Math 130 Lecture Notes General

I'm shifting my math notes to this location and condensing them a bit.  This file will also host general notes.  The lectures are a bit miserable...
[[session-3-notes]]
[[session-4-notes]]

---
**Session start**
Something about the project mentioned right at the start of the session...  I still don't know what to expect from the projects in this class.  Oh dear, it's something in the online service that is already opened to be worked on.  Forty questions long and somewhat enigmatic...  I suppose I'll have to email the professor for information, though I'll start by trying to puzzle it out myself.  At least something is being talked about that isn't interminable discussion of the properties of r.

The project seems to differentiate from the homework by granting only one attempt per question whereas the projects and tests allow only one attempt per question.  I used enough of the multiple attempts in the homework that this is a meaningful increase in the difficulty.  I used some of those attempts for typographical errors, which magnifies the stress...  Maybe I should imagine a brain implant that's nothing but prewired awesome typistry... Now there's a skillsoft that would help a lot of people in minor ways to accelerate their life without accelerating their stress levels.

{Transcribed from lecture}
"Chapter 4 is all about relationships and regressions."

{Transcribed from slide}
Objectives
Regression
* The least-square regression line
* Facts about least-square regression
* (I missed one)

{Transcribed from slide}
The least-squares regression line is the line that makes the sum of the squared vertical distances of the data points from the line as small as possible.
(Chart depicting "Blood Alcohol Content as a function of Number of Beers")
(Section highlighted from the chart, depicting a dot above a line and a dot below that line at the same point)
Observed y=0.070
distance to line = y-y^ = 0.032
Predicted y^ = 0.048
distance to line = y - y^ = 0.028
observed y = 0.020

I'm not sure what this symbol I've depicted here as y^ actually is.  The teacher called it "y-half", eliding over it.  I need to be spending more time studying my statistics book...  Oh, I misheard.  She's calling it y-hat.

y^ = a+bx

(Transcribed from slide)
Residuals
(Unlabeled graph)
Points above the line have a positive residual (under estimation).
Points below the line have a negative residual (over estimation).

(Transcribed from slide)
y^ is the predicted y value on the regression line.
y^ = intercept + slope x
y^ = a + bx
Stat-Calc- 8. LinReg (a+bx)
y^ = a + bx
a=
b=
r^2=
r=
(Unlabeled extremely simple graph, pointing down)
b = slope < 0
r < 0
(Horizontal line in a box)
b = slope = 0
r = 0
(Unlabeled extremely simple graph, pointing up)
b = slope > 0
r > 0

I'm the only one with my camera on, and I guess I'll change that.

[//begin]: # "Autogenerated link references for markdown compatibility"
[session-3-notes]: session-3-notes "Session 3 Notes"
[session-4-notes]: session-4-notes "Session 4 Notes"
[//end]: # "Autogenerated link references"